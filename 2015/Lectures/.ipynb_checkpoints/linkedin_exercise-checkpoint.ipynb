{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## all imports\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import bs4 #this is beautiful soup\n",
    "import time\n",
    "import operator\n",
    "import socket\n",
    "import cPickle\n",
    "import re # regular expressions\n",
    "import csv\n",
    "\n",
    "\n",
    "from pandas import Series\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "#global definitions\n",
    "url_gs = 'https://www.bing.com/search?q=%2bhsbc+site%3awww.linkedin.com%2fin%2f&qs=n&pq=%2bhsbc+site%3awww.linkedin.com%2fin%2f&sc=0-31&sp=-1&sk=&cvid=31462396F9D2434DBD4F86F5AF86CEA7'\n",
    "hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def result_pages_count():\n",
    "    # let's get the total number of results\n",
    "    source_gs = urllib2.urlopen(url_gs).read()\n",
    "    # parse html code\n",
    "    bs_tree_gs = bs4.BeautifulSoup(source_gs,\"lxml\")\n",
    "\n",
    "    content_result = bs_tree_gs.find(id='b_content')\n",
    "    result_count = content_result.getText('sb_count')\n",
    "    result_count = re.match('(?P<total>[0-9]+\\.[0-9]+)', result_count)\n",
    "    count = result_count.group('total')\n",
    "    \n",
    "    count = re.sub('\\.','',count)\n",
    "    count = re.sub('\\,','',count)\n",
    "    return int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_result_items(page):\n",
    "     \n",
    "    name_list = []\n",
    "    link_list = []\n",
    "    search_link_big = url_gs+'&first='+ str(page)+\"&FORM=PORE\"\n",
    "    \n",
    "    print \"[info] obtaining page \" + str(page) + \" | \" + search_link_big\n",
    "    \n",
    "    source_gs = urllib2.urlopen(search_link_big).read()\n",
    "    # parse html code\n",
    "    bs_tree_gs = bs4.BeautifulSoup(source_gs,\"lxml\")\n",
    "\n",
    "    # see how many job postings we found\n",
    "    results = bs_tree_gs.find(id ='b_results')\n",
    "    result_items = results.findAll('li')\n",
    "    \n",
    "    result_items = [jp for jp in result_items if not jp.get('class') is None \n",
    "                    and ''.join(jp.get('class')) ==\"b_algo\"]\n",
    "\n",
    "    for i in result_items:\n",
    "        #i.encode(\"utf-8\")\n",
    "        tag_a = i.find('a')\n",
    "        link_list.append(tag_a.get('href').encode(\"utf-8\"))\n",
    "        name_list.append(tag_a.getText().encode(\"utf-8\"))\n",
    "        \n",
    "    enum_items = zip(name_list, link_list)\n",
    "    \n",
    "    return enum_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_bing_search_to_file():\n",
    "    count = int(result_pages_count())\n",
    "    # count = 1 para evitar acidentes...\n",
    "    count = 1\n",
    "    print \"Pesquisa encontrou: \",count\n",
    "\n",
    "    index = 1 \n",
    "    retry = 0\n",
    "    with open('C:/projetos/cs109_marstival/user_linkedin.csv', 'w') as out:\n",
    "        writer = csv.writer(out,delimiter=';')\n",
    "\n",
    "        while index < count:\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                result_tuples = read_result_items(index)\n",
    "            except Exception, e:\n",
    "                if retry >= 2:\n",
    "                    retry = 0\n",
    "                    print(\"[fatal_error] time out... retrying (%d) in a few seconds\", retry, e)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"[error] time out... retrying (%d) in a few seconds\", retry, e)\n",
    "                    retry += 1\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "\n",
    "            #for u,l in result_tuples:\n",
    "\n",
    "            for name, link in result_tuples:\n",
    "                name = name.replace(\" | LinkedIn\",\"\")\n",
    "                #print name\n",
    "                writer.writerow([index, name, link])\n",
    "\n",
    "                index += 1\n",
    "        out.close()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c02983e20e3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "print index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_profile_positions(bs_tree_linkedin):\n",
    "    \n",
    "    experience_section = bs_tree_linkedin.find(id ='experience')\n",
    "    if experience_section is None:\n",
    "        return None\n",
    "    \n",
    "    position_list = experience_section.findAll('li')\n",
    "    \n",
    "    if position_list is None:\n",
    "        return None\n",
    "    \n",
    "    user_data =[]\n",
    "    i=0\n",
    "    for position in position_list:\n",
    "        start= end= \"\"\n",
    "        \n",
    "        data_section = position.get('data-section')\n",
    "\n",
    "        period = position.findAll('time')\n",
    "        times = len (period)\n",
    "        if (times > 0):\n",
    "            start = period[0].getText()\n",
    "        if times > 1: \n",
    "            end = period[1].getText()\n",
    "\n",
    "        employer = position.find('h5',attrs='item-subtitle').getText()\n",
    "        i+=1\n",
    "        user_data.append([data_section, employer.encode(\"utf-8\"), start, end, i ])\n",
    "\n",
    "    \n",
    "    return user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_profile_info(bs_tree_linkedin):\n",
    "    \n",
    "    linkedin_profile = bs_tree_linkedin.find(id ='topcard')\n",
    "    if linkedin_profile is None:\n",
    "        print \"[warning] extract_profile_info() linkedin profile has no 'topcard' (summary info)\\n\"\n",
    "        return None\n",
    "    \n",
    "    extra_info = linkedin_profile.find('table')\n",
    "    if extra_info is None or re.search(\"extra-info\", str(extra_info.get('class'))) is None:\n",
    "        print \"[warning] extract_profile_info() linkedin profile has no extra_info \\n\"\n",
    "        return None\n",
    "   \n",
    "    currentPosition= previousPosition= []\n",
    "\n",
    "    positions_section = extra_info.findAll('tr')\n",
    "    #print info_section\n",
    "    for content in positions_section:\n",
    "        data_section = content.get('data-section')\n",
    "        if data_section == 'currentPositionsDetails':\n",
    "                positions = content.findAll('li')\n",
    "                currentPosition = [s.getText().encode(\"utf-8\") for s in   positions]\n",
    "                #print \"extract_profile_info() currentPosition\", currentPosition, '\\n'\n",
    "        elif data_section == 'pastPositionsDetails':\n",
    "                positions = content.findAll('li')\n",
    "                previousPosition = [s.getText().encode(\"utf-8\") for s in   positions]\n",
    "                #print \"extract_profile_info() previousPosition\", previousPosition, '\\n'\n",
    "        else:\n",
    "            None\n",
    "            #print '[info] discarding %s section', content.get('data-section'), content.getText()\n",
    "    \n",
    "    name = \"\"\n",
    "    name_section = linkedin_profile.find(id='name')\n",
    "    if (name_section is not None):\n",
    "        name = name_section.getText().encode(\"utf-8\")\n",
    "        \n",
    "    headline = \"\"\n",
    "    info_section = linkedin_profile.find('p')\n",
    "    if (info_section is not None):\n",
    "        headline = info_section.getText().encode(\"utf-8\")\n",
    "    \n",
    "    location = \"\"\n",
    "    demographics = linkedin_profile.find(id='demographics')  \n",
    "    location_section = demographics.find('dd')\n",
    "    if (location_section is not None):\n",
    "        location = location_section.getText().encode(\"utf-8\")\n",
    "    \n",
    "    return [name, headline, location, currentPosition, previousPosition ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_linkedin_info(url_linkedin):\n",
    "    #name_linkedin, url_linkedin = name_link_pair\n",
    "    \n",
    "    req = urllib2.Request(url_linkedin, headers=hdr)\n",
    "\n",
    "    source_linkedin = urllib2.urlopen(req).read()\n",
    "    # parse html code\n",
    "    bs_tree_linkedin = bs4.BeautifulSoup(source_linkedin,\"lxml\")\n",
    "\n",
    "    #lista (coluns)    \n",
    "    profile_info = extract_profile_info(bs_tree_linkedin)\n",
    "    #list de lista (linhas/colunas)\n",
    "    profile_positions = extract_profile_positions(bs_tree_linkedin)\n",
    "\n",
    "#    print profile_info, '\\n'\n",
    "#    print profile_positions, '\\n'\n",
    "    \n",
    "    return profile_info, profile_positions\n",
    "#print linkedin_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_profile(row, profile_info):\n",
    "    data_to_write = []\n",
    "    #id\n",
    "    data_to_write.append(row[0])\n",
    "    #name\n",
    "    data_to_write.append(row[1])\n",
    "    #name2\n",
    "    data_to_write.append(profile_info[0])\n",
    "    #headline\n",
    "    data_to_write.append(profile_info[1])\n",
    "    #location\n",
    "    data_to_write.append(profile_info[2])\n",
    "    #link\n",
    "    data_to_write.append(row[2])\n",
    "    return data_to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data_positions_summary(row, profile_info):\n",
    "    data_to_write = []\n",
    "    \n",
    "    currentPositions = profile_info[3]\n",
    "    previousPositions = profile_info[4]\n",
    "    \n",
    "    len_current = len(currentPositions)\n",
    "    len_previous = len(previousPositions)\n",
    "    types = ([\"currentPositions\"] * len_current) + ([\"previousPositions\"] * len_previous )\n",
    "    #listas \n",
    "    data_to_write = zip( ([row[0]]* (len_current+len_previous)), (currentPositions + previousPositions), types )\n",
    "    \n",
    "    return data_to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_positions_complete(row, profile_positions):\n",
    "        \n",
    "    for l in profile_positions:\n",
    "        l.insert(0, row[0])\n",
    "        \n",
    "    data_to_write = profile_positions\n",
    "    \n",
    "    return data_to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[error] error obtaining employer information. skip  28 HSBC Gale Issitt https://www.linkedin.com/in/hsbc-gale-issitt-5a263a4 HTTP Error 404: Not Found\n",
      "[error] error obtaining employer information. skip  60 Erlangga HSBC https://www.linkedin.com/in/erlangga-hsbc-3338871b HTTP Error 404: Not Found\n",
      "[error] error obtaining employer information. skip  73 Rodrigo Hsbc https://www.linkedin.com/in/rodrigo-hsbc-6499b027 HTTP Error 404: Not Found\n",
      "[error] error obtaining employer information. skip  515 陳志堅--HsbcAdam https://www.linkedin.com/in/adam-陳志堅-hsbc-7452b441 HTTP Error 404: Not Found\n",
      "[error] error obtaining employer information. skip  774 Srinivas hsbc 1 https://www.linkedin.com/in/srinivas-hsbc-1-519894118 HTTP Error 404: Not Found\n",
      "[error] error obtaining employer information. skip  857 Meri Hsbc Hsbc https://www.linkedin.com/in/meri-hsbc-hsbc-593a95a8 HTTP Error 404: Not Found\n",
      "[warning] extract_profile_info() linkedin profile has no 'topcard' (summary info)\n",
      "\n",
      "[error] error obtaining employer information. skip  48584 Holly Colins https://www.linkedin.com/in/holly-colins-78a22211b HTTP Error 404: Not Found\n",
      "[error] error obtaining employer information. skip  50605 Sayed Al Edroos HSBC https://www.linkedin.com/in/sayed-al-edroos-hsbc-97b18a88 HTTP Error 404: Not Found\n",
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "#arquivos de entrada (index, nome, link)\n",
    "f_profiles_name_link = 'C:/projetos/cs109_marstival/user_linkedin_all - deduplicated.csv'\n",
    "#arquivos de saida\n",
    "f_info = 'C:/projetos/cs109_marstival/linkedin_profile_info.csv'\n",
    "f_pos_summary = 'C:/projetos/cs109_marstival/linkedin_profile_positions.csv'\n",
    "f_pos_complete = 'C:/projetos/cs109_marstival/linkedin_profile_positions_details.csv'\n",
    "\n",
    "with open(f_info, 'w') as out_info, open(f_pos_summary,'w') as out_p_summary, open(f_pos_complete, 'w') as out_p_complete:\n",
    "    writer_info = csv.writer(out_info,delimiter=';')\n",
    "    writer_p_summary = csv.writer(out_p_summary,delimiter=';')\n",
    "    writer_p_complete = csv.writer(out_p_complete,delimiter=';')\n",
    "    \n",
    "    with open(f_profiles_name_link,'r') as users_file:\n",
    "        reader = csv.reader(users_file,delimiter=';')\n",
    "        \n",
    "        for row in reader:\n",
    "            if len(row) < 3: \n",
    "                print \"[warning] row read from user_linkedin has < 2 columns\"\n",
    "                continue;\n",
    "            #index, name, link\n",
    "            try:\n",
    "                time.sleep(2)\n",
    "                profile_info, profile_positions = extract_linkedin_info(row[2])\n",
    "            except Exception, e:\n",
    "                print \"[error] error obtaining employer information. skip \", row[0], row[1], row[2], e\n",
    "                continue;\n",
    "            \n",
    "            #print \"profile_positions\",profile_positions\n",
    "            \n",
    "            if profile_info is not None:\n",
    "                data_to_write = prepare_data_profile(row, profile_info)\n",
    "                #print data_to_write, '\\n'\n",
    "                writer_info.writerow(data_to_write)\n",
    "            \n",
    "            if profile_info is not None:\n",
    "                data_to_write = prepare_data_positions_summary(row, profile_info)\n",
    "                #print data_to_write\n",
    "                #ROWS list of list\n",
    "                writer_p_summary.writerows(data_to_write)\n",
    "            \n",
    "            if profile_positions is not None:  \n",
    "                data_to_write = prepare_data_positions_complete(row, profile_positions)\n",
    "                #print data_to_write\n",
    "                #ROWS list of list\n",
    "                writer_p_complete.writerows(data_to_write)\n",
    "            \n",
    "        users_file.close()\n",
    "    out_info.close()\n",
    "    out_p_summary.close()\n",
    "    out_p_complete.close()\n",
    " \n",
    "    print \"Extraction complete!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = None\n",
    "\n",
    "if a is None:\n",
    "    a = \"a\"\n",
    "\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
